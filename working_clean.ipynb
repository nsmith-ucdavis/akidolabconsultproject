{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import keras\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn import naive_bayes, metrics\n",
    "import sklearn\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in test and training sets from createdbalanced_1556_2019 and create_unbalanced_test_400_2018\n",
    "\n",
    "train = pd.read_csv('C:/Users/nateb/Desktop/Insight/Akidolabs/Akido_DPS_Data/train_1556_labeled.csv')\n",
    "trainorig = train\n",
    "test = pd.read_csv('C:/Users/nateb/Desktop/Insight/Akidolabs/Akido_DPS_Data/test_400_2018.csv')\n",
    "testorig = train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine train and test sets, create new column with designator\n",
    "train['which'] = \"train\"\n",
    "test['which'] = \"test\"\n",
    "new=train.append(test) \n",
    "train=new\n",
    "#print(train.shape)\n",
    "#print(train['which'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##rename PoliceInformationSummary variable to pis\n",
    "##make all letters lowercase in combined data\n",
    "train = train.rename(columns={\"PoliceInformationSummary\": \"pis\"})\n",
    "train['pis'] = train['pis'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use keras to convert dispatch calls to series of words/tokens\n",
    "train['clean_pis'] = train['pis'].apply(lambda x: ' '.join(text_to_word_sequence(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare first entry for police information summary before and after \n",
    "#text_to_word_sequence\n",
    "train['clean_pis'].iloc[0],train['pis'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##regular expression to remove unwanted character strings\n",
    "##function to run all re's\n",
    "def standardize_text(df, text_field):\n",
    "\n",
    "    #replace links\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    #Remove non alphanumeric\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    #Replace @ with at\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    #Delete all digits greater than 1 numbers\n",
    "    df[text_field] = df[text_field].str.replace(r\"\\d{1,}\", \"\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply function to cleaned pis column\n",
    "train = standardize_text(train, 'clean_pis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare first entry for police information summary before and after \n",
    "#re's\n",
    "train['clean_pis'].iloc[0],train['pis'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer for cleaned text using NLTK\n",
    "#loop through all dispatch entries\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "train['clean_pis_lemmatized'] = train.clean_pis.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare first entries for police information summary \n",
    "#at different cleaning stages\n",
    "train['pis'].iloc[0],train['clean_pis'].iloc[0],train['clean_pis_lemmatized'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resplit into test and train after cleaning\n",
    "#confirm dimensions\n",
    "test = train[train['which'] == 'test']\n",
    "train = train[train['which'] == 'train']\n",
    "print(test.shape)\n",
    "print(train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tabulate values in manual class column\n",
    "#this column contains labels that I manually entered for both test and train sets\n",
    "#where 9 appears, I was unsure whether the entry had to to do with homelessness\n",
    "print(train['manual_class'].value_counts())\n",
    "print(test['manual_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Replace 9 with 0, unsure, to not homeless\n",
    "test['manual_class'] = test['manual_class'].replace(9,0)\n",
    "train['manual_class']= train['manual_class'].replace(9,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm that conversion has worked\n",
    "print(train['manual_class'].value_counts())\n",
    "print(test['manual_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the dispatch text using TF-IDF for test and training sets\n",
    "#Get and store feature Names        \n",
    "#Remove stop words\n",
    "#Print out number of vectorized features\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "fitted_name = tfidf_vec.fit(train['clean_pis'])\n",
    "feature_name = fitted_name.get_feature_names()\n",
    "print (len(feature_name))\n",
    "\n",
    "fitted_name_test = tfidf_vec.fit(test['clean_pis'])\n",
    "feature_name_test = fitted_name_test.get_feature_names()\n",
    "print (len(feature_name_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/transform training set, transform only test\n",
    "#Fit_transform training text\n",
    "#transform only test text\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(stop_words = 'english')\n",
    "\n",
    "fitted = train['clean_pis'].tolist()\n",
    "train_x=tfidf_vec.fit_transform(fitted)\n",
    "train_y = train['manual_class']\n",
    "\n",
    "\n",
    "fitted_test = test['clean_pis'].tolist()\n",
    "test_x=tfidf_vec.transform(fitted_test)\n",
    "test_y = test['manual_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(train_x.toarray(), columns=feature_name).iloc[:,0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train model using Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "%time nb.fit(train_x, train_y)\n",
    "from sklearn import metrics\n",
    "y_pred_class = nb.predict(test_x)\n",
    "metrics.accuracy_score(test_y, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_y.value_counts())\n",
    "\n",
    "#Compute null accuracy\n",
    "null_accuracy = test_y.value_counts().head(1) / len(test_y)\n",
    "print('Null accuracy:', null_accuracy)\n",
    "\n",
    "# Manual calculation of null accuracy by always predicting the majority class\n",
    "print('Manual null accuracy:',(374 / (374 + 26)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##confusion matrix for naive bayes classifier\n",
    "metrics.confusion_matrix(test_y, y_pred_class)\n",
    "#Confusion matrix\n",
    "#[TN FP\n",
    "#FN TP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print message text for the false positives\n",
    "o_x = test['clean_pis']\n",
    "# collection of false positives\n",
    "false_positives = o_x[(y_pred_class==1) & (test_y==0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displey false positive at position [8]\n",
    "false_positives.iloc[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print message text for the false negatives \n",
    "o_x = test['clean_pis']\n",
    "#o_x[y_pred_class < y_test]\n",
    "# alternative less elegant but easier to understand\n",
    "false_negatives = o_x[(y_pred_class==0) & (test_y==1)]\n",
    "print(false_negatives.iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#####  Logistic Regression ##########\n",
    "#####################################\n",
    "\n",
    "##Compare using logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 2. instantiate a logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "# 3. train the model using X_train_dtm\n",
    "%time logreg.fit(train_x, train_y)\n",
    "# 4. make class predictions for X_test_dtm\n",
    "y_pred_class_log = logreg.predict(test_x)\n",
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "y_pred_prob = logreg.predict_proba(test_x)[:, 1]\n",
    "y_pred_prob\n",
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(test_y, y_pred_class_log))\n",
    "# calculate AUC\n",
    "print(metrics.roc_auc_score(test_y, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all probabilities for both 0 and 1 classes, 2d array\n",
    "logreg.predict_proba(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicted class for test set\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusiong matrix for logistic regression classifier \n",
    "metrics.confusion_matrix(test_y, y_pred_class_log)\n",
    "#Confusion matrix\n",
    "#[TN FP\n",
    "#FN TP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print actual values vs predicted values\n",
    "pred = np.array(y_pred_class_log).tolist()\n",
    "prob = np.array(y_pred_prob).tolist()\n",
    "adj = pd.DataFrame({'actual': test_y, 'prob': prob, 'pred': pred})\n",
    "print(adj['actual'].value_counts())\n",
    "print(adj['pred'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output predictions to csv\n",
    "adj.to_csv (r'C:\\Users\\nateb\\Desktop\\Insight\\Akidolabs\\Akido_DPS_Data\\pred.csv', \n",
    "                        index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#False positives\n",
    "o_x = test['clean_pis']\n",
    "false_positives_log = o_x[(y_pred_class_log==1) & (test_y==0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#false negatives\n",
    "o_x = test['clean_pis']\n",
    "false_negatives_log = o_x[(y_pred_class_log==0) & (test_y==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print false negative in position [2] for logistic classifier\n",
    "print(false_negatives_log.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# roc curve and auc score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify Plot ROC curve details\n",
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for Homeless Classification')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(test_y, y_pred_prob)\n",
    "print('AUC: %.2f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(test_y, y_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ROC curve\n",
    "plot_roc_curve(fpr, tpr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
